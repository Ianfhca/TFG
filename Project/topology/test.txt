-- PARAMETERS --

# Time stages (m, s, ms, us, ns) 
#time 100 ms
#dt 1000 us
dt 1 ms
#dt 100 us
#time 100 ms
time 10 s
#dt 10000 us
#Integer value gratter than 0
>

-- HYPERPARAMETERS --

# Type 0 (Default)
v_reset -65.0
v_rest -65.0
v -65.0
#v_th -64.0
v_th -50.0
tau_m 20.0
t_refr 2 ms
lambda_x 20.0
alpha 0.1
weight 0.5
#learning_rate 0.0001
learning_rate 0.1
a 0.0
convergence_th 0.05
#convergence_th 0.01
>

-- TOPOLOGY --

# Layer 0 (Input Layer)
layer Input
neurons 4 4 2 #(Height x Width x Channels)
type 0
connections none
multisynapses 1
stdp 0
wta 0
delay 100 150 us #(Same unit as dt)
>

# Layer 1 (Feature Extraction)
layer SS-Conv
neurons 0
type 0
connections local
kernel_size 2 #(window size rxr)
kernels_amount 1 #(output maps k)
multisynapses 1
stdp 0
wta 1
delay 1 2 ms
#delay 100 150 us
>

# Layer 5 (Global Motion Perception)
layer Output
neurons 11 1 1
type 0
connections dense
multisynapses 1
stdp 1
wta 1
delay 1 1 ms
#delay 100 150 us
>

#----------------------------------------------------------------------------------------------------------------
# Falta realizar correctamente las sparse connections

# Sparse connections de capa 0 a capa 1

#connections sparse
#sparse_connection 0 0
#sparse_connection 1 1
#sparse_connection 2 1
#sparse_connection 3 0

# Tener en cuenta los delays acumulados para la finalización de la simulación

#lamdaX = -1/tau_m

# Comprobar que la forcing function no sea += en vez de = ·
# Arreglar delays ·
# Funcionamiento multisinapsis ·
# Corregir updateNeuron. Mirar si forcing function se tiene que hacer para multisyinapsis tambn. for(i; i < multisyinapsis; i++) ·
# Separar ficheros datos y estructura ·
# Enlazar dataset ·
# STDP (Backpropagation) ·

# Cambiar nombres tau etc. para los delays (Jose) ·

# a and learning_rate as parameters ·

# Falta formula MSE del STDP. ·

# TODO ------------------------------------------------------------------------------------- Modificar t del membrane potential

# Documentar cada método
# Gráficos


# Falta hacer bucle para recorrer todos los gestos de todos los ficheros de train.
# Falta ajustar el dt para que coincida con el dt pasado por parametro.

# Falta guardar pesos en fichero binario e implementar test.
# Falta salida de la red ????
# Falta graficos (mapa de calor, MSE...)

# Preguntas JOSE:
# El forcing function hay que sumarlo?
# Que es la salida de la red? - Simplemente guardar las spikes de salida
# Limitaciones pesos y trazas preX? (Sinapsis.cpp entre 0 y 1) Apendice B paper - No
# El ganador es el que actualiza el STDP solamente? - Si
# Las locally connected neurons comparten pesos? Hay que tener una W en neuron en la que se haga el dot product de todas las synapsis? - No


# Los setWeight() hay que sumar o no? += ·
# la neurona 13078 deja de aprender ·
# la traza presinaptica y los pesos se están normalizando mal puede ·
# corregir el input para que pueda tener cualquier dimensión, no solo 128x128 SpikeCubePolarity ·
# corregir los parametros de la topologia ·


# Revisar si hay que hacer preNeuronShared->setSpike(0); ·

# Falta quitar el STDP en test ·

# Pasar todo el tiempo a ms o us?? (dt)
# El paso del tiempo dt(t) es respecto a cada gesto o debe ser unico [0-infinity]?? En base a esto cambiar tRefr también 
# Tener en cuenta el dt entre cada gesto, si no se resetea a 0 tras cada gesto tal vez se pierda localidad espacial. Si no, tener en cuenta que lastSpike puede ser mayor que t.
# Revisar formulas
# Exponencial nunca dispara | -dt/tauM pasa de -65 a 3100 instantaneo | -1/tauM pasa de -65 a -47 instantaneo
# Puedo usar la foto de la neurona del paper?
# En el WTA, la que gana hace STDP() y las demas se inhiben inmediatamente o se inhiben despues de haber comprobado toda la capa? Esto supone que una neurona gane más de una vez y acapare toda la atención, pero si no, ciertas neuronas quedan incapacitadas para competir en la siguiente ventana.

# // synapses[i]->updateSpikeAtributes();
#                    s = synapses[i]->obtainSpike();
 #                   synapses[i]->updatePresinapticTrace(0);
  #                  forcingFunction += synapses[i]->updateForcingFunction(0);

# prints para ver el dataset, el maxpre y minpre y la forcing function y la membrana